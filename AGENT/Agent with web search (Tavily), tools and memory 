import json
import os
import requests
from typing import Annotated, TypedDict
from dotenv import load_dotenv

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

from langchain.chat_models import init_chat_model
from langchain_core.tools import tool
from langchain_core.messages import ToolMessage
from langchain_tavily import TavilySearch


# === Load environment variables ===
load_dotenv()
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if TAVILY_API_KEY is None:
    raise ValueError("TAVILY_API_KEY environment variable not set.")
if OPENAI_API_KEY is None:
    raise ValueError("OPENAI_API_KEY environment variable not set.")


# === Define Tools ===

@tool
def get_forecast_predictions(target: str, pam_position: int) -> dict:
    """Retrieve predictions from the Elixir Forecast API for a CRISPR sequence."""
    url = "https://elixir.ut.ee/forecast/api/predict"
    payload = {"target": target, "pam_position": pam_position}
    headers = {"Content-Type": "application/json"}
    response = requests.post(url, json=payload, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        return {"error": f"Status code {response.status_code}: {response.text}"}


@tool
def get_crisprs_by_exon(species: str, exon_ids: list[str]) -> dict:
    """Retrieve CRISPR guides for a species and list of exon IDs using the WGE API."""
    url = "https://wge.stemcell.sanger.ac.uk/api/crispr_search"
    params = {"species": species}
    for exon_id in exon_ids:
        params.setdefault("exon_id[]", []).append(exon_id)

    response = requests.get(url, params=params)
    if response.status_code == 200:
        raw_data = response.json()
        processed = {}
        for exon_id, guides in raw_data.items():
            processed[exon_id] = []
            for guide in guides:
                species_name = (
                    "Mouse" if guide.get("species_id") == 2 else
                    "Human" if guide.get("species_id") == 4 else
                    guide.get("species_id")
                )
                processed[exon_id].append({
                    "id": guide.get("id"),
                    "chr_name": guide.get("chr_name"),
                    "chr_start": guide.get("chr_start"),
                    "chr_end": guide.get("chr_end"),
                    "seq": guide.get("seq"),
                    "pam_right": guide.get("pam_right"),
                    "ensembl_exon_id": guide.get("ensembl_exon_id"),
                    "off_target_summary": guide.get("off_target_summary"),
                    "exonic": guide.get("exonic"),
                    "species": species_name
                })
        return processed
    else:
        return {"error": f"Status code {response.status_code}: {response.text}"}


# === Tavily Search Tool ===
tavily_tool = TavilySearch(max_results=2)

# === Tool List ===
tools = [get_forecast_predictions, get_crisprs_by_exon, tavily_tool]


# === State Schema ===
class State(TypedDict):
    messages: Annotated[list, add_messages]


# === Initialize Chat Model and Bind Tools ===
llm = init_chat_model("openai:gpt-4")
llm_with_tools = llm.bind_tools(tools)

# === In-Memory Checkpoint (Memory) ===
memory = MemorySaver()

# === LangGraph Construction ===
graph_builder = StateGraph(State)


# === Chatbot Node ===
def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)


# === Custom ToolNode ===
class BasicToolNode:
    def __init__(self, tools: list) -> None:
        self.tools_by_name = {tool.name: tool for tool in tools}

    def __call__(self, inputs: dict):
        if messages := inputs.get("messages", []):
            message = messages[-1]
        else:
            raise ValueError("No message found in input")
        outputs = []
        for tool_call in message.tool_calls:
            tool_result = self.tools_by_name[tool_call["name"]].invoke(tool_call["args"])
            outputs.append(
                ToolMessage(
                    content=json.dumps(tool_result),
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"],
                )
            )
        return {"messages": outputs}

tool_node = BasicToolNode(tools=tools)
graph_builder.add_node("tools", tool_node)


# === Router for Conditional Tool Execution ===
def route_tools(state: State):
    messages = state.get("messages", [])
    ai_message = messages[-1] if messages else None
    if hasattr(ai_message, "tool_calls") and ai_message.tool_calls:
        return "tools"
    return END


# === Define Graph Edges ===
graph_builder.add_conditional_edges("chatbot", route_tools, {"tools": "tools", END: END})
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")

# === Compile Graph with Memory ===
graph = graph_builder.compile(checkpointer=memory)


# === Run Chatbot in CLI Loop ===
def stream_graph_updates(user_input: str):
    state = {"messages": [{"role": "user", "content": user_input}]}
    result = graph.invoke(state, config={"configurable": {"thread_id": "1"}})
    print(f"\nðŸ¤– Assistant: {result['messages'][-1].content}\n")


if __name__ == "__main__":
    print("ðŸ”¬ CRISPR Chatbot with Tools and Memory\nType 'quit' to exit.")
    while True:
        try:
            user_input = input("ðŸ§‘ User: ")
            if user_input.lower() in {"quit", "exit", "q"}:
                print("ðŸ‘‹ Goodbye!")
                break
            stream_graph_updates(user_input)
        except KeyboardInterrupt:
            print("\nðŸ‘‹ Goodbye!")
            break
